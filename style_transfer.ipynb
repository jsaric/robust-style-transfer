{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from resnet import resnet50\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dir = Path(\"./input/content/\")\n",
    "style_dir = Path(\"./input/style/\")\n",
    "model_dir = Path(\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gram matrix and loss\n",
    "def gram_matrix(x):\n",
    "    b,c,h,w = x.size()\n",
    "    F = x.view(b, c, h*w)\n",
    "    G = torch.bmm(F, F.transpose(1,2)) \n",
    "    G.div_(h*w)\n",
    "    return G\n",
    "\n",
    "def gram_mse(x, gt):\n",
    "    return F.mse_loss(gram_matrix(x), gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre(post)processing\n",
    "img_size = 512\n",
    "\n",
    "transforms_fw = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "         mean=[0.48501961, 0.45795686, 0.40760392], \n",
    "         std=[0.2290, 0.2240, 0.2250])\n",
    "])\n",
    "\n",
    "transforms_bw = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.mul_(torch.tensor([0.2290, 0.2240, 0.2250]).view(3, 1, 1))),\n",
    "    transforms.Lambda(lambda x: x.add_(torch.tensor([0.48501961, 0.45795686, 0.40760392]).view(3, 1, 1))),\n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "    transforms.ToPILImage()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image and style reconstruction functions\n",
    "def reconstruct(img_path, model, layer, max_iter=5000, show_iter=100):\n",
    "    img = transforms_fw(Image.open(img_path)).unsqueeze(0).cuda()\n",
    "    target = model.forward_layers(img, [layer])[0]\n",
    "    opt_img = torch.randn(img.size()).type_as(img.data).requires_grad_(True) #random init\n",
    "    optimizer = optim.Adam([opt_img], lr=5e-2)\n",
    "    print(\"Reconstruction optimization process started...\")\n",
    "    loss_history = []\n",
    "    for i in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        x = model.forward_layers(opt_img, [layer])[0]\n",
    "        loss = F.mse_loss(x, target)\n",
    "        loss.backward()\n",
    "        if i%show_iter == 0 and i != 0:\n",
    "            print('Iteration: %d, loss: %f'%(i, loss.item()))\n",
    "        optimizer.step()\n",
    "        loss = loss.cpu().item()\n",
    "        if len(loss_history) > 25 and np.isclose(np.array(loss_history[-25:]), loss).all():\n",
    "            break\n",
    "        loss_history += [loss]\n",
    "    \n",
    "    \n",
    "    #display result\n",
    "    out_img = transforms_bw(opt_img.data[0].cpu().squeeze())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out_img)\n",
    "    return out_img\n",
    "\n",
    "def style_reconstruct(img_path, model, layer, max_iter=5000, show_iter=100):\n",
    "    img = transforms_fw(Image.open(img_path)).unsqueeze(0).cuda()\n",
    "    target = model.forward_layers(img, [layer])[0]\n",
    "    target = gram_matrix(target)\n",
    "    opt_img = torch.randn(img.size()).type_as(img.data).requires_grad_(True) #random init\n",
    "    optimizer = optim.Adam([opt_img], lr=5e-2)\n",
    "    loss_history = []\n",
    "    print(\"Style reconstruction optimization process started...\")\n",
    "    for i in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        x = model.forward_layers(opt_img, [layer])[0]\n",
    "        x = gram_matrix(x)\n",
    "        loss = 1000 * F.mse_loss(x, target)\n",
    "        loss.backward()\n",
    "        if i%show_iter == 0 and i != 0:\n",
    "            print('Iteration: %d, loss: %f'%(i, loss.item()))\n",
    "        optimizer.step()\n",
    "        loss = loss.cpu().item()\n",
    "        if len(loss_history) > 25 and np.isclose(np.array(loss_history[-25:]), loss).all():\n",
    "            break\n",
    "        loss_history += [loss]\n",
    "    \n",
    "    #display result\n",
    "    out_img = transforms_bw(opt_img.data[0].cpu().squeeze())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out_img)\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(\n",
    "        content_img_path, \n",
    "        style_img_path, \n",
    "        model, \n",
    "        content_layers=[\"layer3\"], \n",
    "        style_layers=[\"layer1\", \"layer2\", \"layer3\", \"layer4\"], \n",
    "        content_weights=[1], \n",
    "        style_weights=[1e4, 1e3, 1e2, 1e1], \n",
    "        max_iter=600, \n",
    "        show_iter=100):\n",
    "    \n",
    "    style_img = transforms_fw(Image.open(style_img_path)).unsqueeze(0).cuda()\n",
    "    content_img = transforms_fw(Image.open(content_img_path)).unsqueeze(0).cuda()\n",
    "    opt_img = content_img.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    loss_layers = style_layers + content_layers\n",
    "    loss_fns = [gram_mse] * len(style_layers) + [F.mse_loss] * len(content_layers)\n",
    "    \n",
    "    #compute optimization targets\n",
    "    style_targets = [gram_matrix(x).detach() for x in model.forward_layers(style_img, style_layers)]\n",
    "    content_targets = [x.detach() for x in model.forward_layers(content_img, content_layers)]\n",
    "\n",
    "    targets = style_targets + content_targets    \n",
    "    weights = style_weights + content_weights\n",
    "    \n",
    "    optimizer = optim.LBFGS([opt_img])\n",
    "    n_iter=[0]\n",
    "    print(\"Style transfer initialized...\")\n",
    "    while n_iter[0] <= max_iter:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward_layers(opt_img, loss_layers)\n",
    "            layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "            loss = sum(layer_losses)\n",
    "            loss.backward()\n",
    "            n_iter[0]+=1\n",
    "            #print loss\n",
    "            if n_iter[0]%show_iter == (show_iter-1):\n",
    "                print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "    out_img = transforms_bw(opt_img.data[0].cpu().squeeze())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out_img)\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model = resnet50(False).cuda().eval()\n",
    "for param in random_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "imagenet_model = resnet50(True).cuda().eval()\n",
    "for param in imagenet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "robust_model = resnet50()\n",
    "robust_state_dict = torch.load(model_dir / \"resnet50_robust.pth\")\n",
    "del robust_state_dict['fc.weight']\n",
    "del robust_state_dict['fc.bias']\n",
    "robust_model.load_state_dict(robust_state_dict, strict=False)\n",
    "robust_model = robust_model.cuda().eval()\n",
    "for param in robust_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in  [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    print(f\"Reconstruction based on features from layer: {l}\")\n",
    "    reconstruct(content_dir / \"kosci-kupres.jpg\", random_model, l, max_iter=500, show_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular ImageNet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in  [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    print(f\"Reconstruction based on features from layer: {l}\")\n",
    "    reconstruct(content_dir / \"kosci-kupres.jpg\", imagenet_model, l, max_iter=500, show_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust ImageNet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in  [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    print(f\"Reconstruction based on features from layer: {l}\")\n",
    "    reconstruct(content_dir / \"kosci-kupres.jpg\", robust_model, l, max_iter=500, show_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in  [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    print(f\"Reconstruction based on features from layer: {l}\")\n",
    "    style_reconstruct(style_dir / \"scream.jpg\", random_model, l, max_iter=5000, show_iter=6000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular ImageNet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in  [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    print(f\"Reconstruction based on features from layer: {l}\")\n",
    "    style_reconstruct(style_dir / \"scream.jpg\", imagenet_model, l, max_iter=5000, show_iter=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust ImageNet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in  [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    print(f\"Reconstruction based on features from layer: {l}\")\n",
    "    style_reconstruct(style_dir / \"scream.jpg\", robust_model, l, max_iter=5000, show_iter=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = content_dir / \"kosci-kupres.jpg\"\n",
    "style_images = [style_dir / p for p in [\"scene_de_rue.jpg\", \"picasso_seated_nude_hr.jpg\" ,\"scream.jpg\" ,\"vangogh_starry_night.jpg\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular ImageNet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for style_img in style_images:\n",
    "    print(f\"Content image: {content_image_path}, Style image: {style_img}\")\n",
    "    style_transfer(content_image_path, style_img, imagenet_model, max_iter=600, show_iter=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust ImageNet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for style_img in style_images:\n",
    "    print(f\"Content image: {content_image_path}, Style image: {style_img}\")\n",
    "    style_transfer(content_image_path, style_img, robust_model, max_iter=600, show_iter=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_images = [content_dir / p for p in [\"drazen-petrovic.jpg\", \"dubrovnik.jpg\", \"rimac.jpeg\", \"fer.jpg\"]]\n",
    "style_images = [style_dir / p for p in [\"contrast_of_forms.jpg\", \"scream.jpg\", \"goeritz.jpg\", \"mondrian_cropped.jpg\"]]\n",
    "\n",
    "for content_image_path, style_image_path in zip(content_images, style_images):\n",
    "    print(f\"Content image: {content_image_path}, Style image: {style_image_path}\")\n",
    "    style_transfer(content_image_path, style_image_path, robust_model, max_iter=600, show_iter=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-dependency on hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image_path = style_dir / \"scream.jpg\"\n",
    "content_image_path = content_dir / \"kosci-kupres.jpg\"\n",
    "x = 100\n",
    "for i in range(6):\n",
    "    style_weights = [x, x * 1e-1, x * 1e-2, x * 1e-3]\n",
    "    print(f\"Style weights: {style_weights}\")\n",
    "    x *= 10\n",
    "    style_transfer(content_image_path, style_image_path, robust_model, max_iter=600, show_iter=700, style_weights=style_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
